import numpy as np
import re
import string
import nltk
import matplotlib.pyplot as plt
from wordcloud import WordCloud,STOPWORDS
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from pandas import DataFrame

from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.stem import WordNetLemmatizer, PorterStemmer

import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
%matplotlib inline
! pip install nltk

nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer

! pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

df = pd. read_csv('covidvaccine1130.csv')
df

print(df.text)

df.dtypes

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyser = SentimentIntensityAnalyzer()
pip install tweepy

#cleaning the tweets
def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, '', input_txt)        
    return input_txt
def clean_tweets(tweets):
    #remove twitter Return handles (RT @xxx:)
    tweets = np.vectorize(remove_pattern)(tweets, "RT @[\w]*:") 
    
    #remove twitter handles (@xxx)
    tweets = np.vectorize(remove_pattern)(tweets, "@[\w]*")
    
    #remove URL links (httpxxx)
    tweets = np.vectorize(remove_pattern)(tweets, "https?://[A-Za-z0-9./]*")
    
    #remove special characters, numbers, punctuations (except for #)
    tweets = np.core.defchararray.replace(tweets, "[^a-zA-Z]", " ")
    
    return tweets
df['text'] = clean_tweets(df['text'])
df['text'].head()


   # initialize the Analyzer

sid = SentimentIntensityAnalyzer()

   # For each sentence get the score


for text in df.text:

    print(text)

    ss = sid.polarity_scores(text)

  # Print the scores

    for k in sorted(ss):
            print('{0}: {1}, '.format(k, ss[k]))
    print()

new_tweet_text = []

for i in range(0, len(tweet_text)):
    new_tweet_text.append(re.sub('http[s]?://\S+\n','', tweet_text[i]))
pd_tweet = pd.DataFrame(new_tweet_text, columns =['text'])
pd_tweet

stop = stopwords.words('english')
description_list=[]
for description in pd_tweet['text']:
    description=re.sub("[^a-zA-Z]", " ", description)
    description=description.lower()
    description=nltk.word_tokenize(description)
    description=[word for word in description if not word in set(stopwords.words("english"))]
    lemma=nltk.WordNetLemmatizer()
    description=[lemma.lemmatize(word) for word in description]
    description=" ".join(description)
    description_list.append(description)
pd_tweet["normalized_text_new"]=description_list
pd_tweet.head(5)

from nrclex import NRCLex 
text_object = NRCLex(' '.join(pd_tweet['normalized_text_new']))
text_object.affect_frequencies

text_object.top_emotions

sentiment_scores = pd.DataFrame(list(text_object.raw_emotion_scores.items())) 
sentiment_scores = sentiment_scores.rename(columns={0: "Sentiment", 1: "Count"})
sentiment_scores

import plotly
import plotly.express as px

fig = px.pie(sentiment_scores, values='Count', names='Sentiment',
             title='Sentiment Scores',
             hover_data=['Sentiment'])
fig.update_traces(textposition='inside', textinfo='percent+label')
fig.show()


from sklearn.feature_extraction.text  import TfidfVectorizer, CountVectorizer

bow_transformer = CountVectorizer().fit(twitter_df['Tweet'])

print (len(bow_transformer.vocabulary_))

twitter_bow = bow_transformer.transform(twitter_df['Tweet'])
from sklearn.feature_extraction.text  import TfidfTransformer

tfidf_transformer = TfidfTransformer().fit(twitter_bow)

# To transform the entire bag-of-words corpus into TF-IDF corpus at once:
   
twitter_tfidf = tfidf_transformer.transform(twitter_bow)

print (twitter_tfidf.shape)

Y = sentiment
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(twitter_tfidf, twitter_df['sentiment'], test_size=0.4, random_state=33)

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(X_train, Y_train)

print("Accuracy on training set: {:.3f}".format(nb.score(X_train, Y_train)))
print("Accuracy on testing set: {:.3f}".format(nb.score(X_test, Y_test)))

from sklearn.metrics import confusion_matrix
nb_y_pred = nb.predict(X_test)

confusion_matrix(Y_test,nb_y_pred)

from sklearn.metrics import classification_report
print(classification_report(Y_test,nb_y_pred))


from wordcloud import WordCloud,STOPWORDS

def word_cloud(wd_list):
    stopwords = set(STOPWORDS)
    all_words = ' '.join([text for text in wd_list])
    wordcloud = WordCloud(
        background_color='white',
        stopwords=stopwords,
        width=1600,
        height=800,
        random_state=1,
        colormap='jet',
        max_words=80,
        max_font_size=200).generate(all_words)
    plt.figure(figsize=(12, 10))
    plt.axis('off')
    plt.imshow(wordcloud, interpolation="bilinear");
word_cloud(df['text'])
